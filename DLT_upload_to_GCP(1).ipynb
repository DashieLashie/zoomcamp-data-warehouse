{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC2QnhmKxpq1"
      },
      "source": [
        "**Please set up your credentials JSON as GCP_CREDENTIALS secrets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UsUZobVduL7l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"DESTINATION__CREDENTIALS\"] = userdata.get(\"GCP_CREDENTIALS\")\n",
        "os.environ[\"BUCKET_URL\"] = \"gs://demo-bucket-dashie\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mPBzsEgyjsBo"
      },
      "outputs": [],
      "source": [
        "# Install for production\n",
        "%%capture\n",
        "!pip install dlt[bigquery, gs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "evdUsDNbkCTk"
      },
      "outputs": [],
      "source": [
        "# Install for testing\n",
        "%%capture\n",
        "!pip install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lYh7r1mTf4uo"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import requests\n",
        "import pandas as pd\n",
        "from dlt.destinations import filesystem\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76zT1PzAgs7A"
      },
      "source": [
        "Ingesting parquet files to GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xya0215jsnsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d22001b-8dcf-4608-a255-bc110273e9e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline rides_pipeline load step completed in 7.44 seconds\n",
            "1 load package(s) were loaded to destination filesystem and into dataset rides_dataset\n",
            "The filesystem destination used gs://demo-bucket-dashie location to store data\n",
            "Load package 1770659058.4247217 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "# Define a dlt source to download and process Parquet files as resources\n",
        "@dlt.source(name=\"rides\")\n",
        "def download_parquet():\n",
        "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
        "    for month in range(1, 7):\n",
        "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
        "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "        # Return the dataframe as a dlt resource for ingestion\n",
        "        yield dlt.resource(df, name=file_name)\n",
        "\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"rides_pipeline\",\n",
        "    destination=filesystem(layout=\"{schema_name}/{table_name}.{ext}\"),\n",
        "    dataset_name=\"rides_dataset\",\n",
        ")\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "load_info = pipeline.run(download_parquet(), loader_file_format=\"parquet\")\n",
        "\n",
        "# Print the results\n",
        "print(load_info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0310FT-gy_P"
      },
      "source": [
        "Ingesting data to Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_3K97w1c2v2",
        "outputId": "5870b76c-8b38-4f48-b117-2033e9a40aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline rides_pipeline load step completed in 26.41 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset rides_dataset\n",
            "The duckdb destination used duckdb:////content/rides_pipeline.duckdb location to store data\n",
            "Load package 1770659240.1576293 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define a dlt resource to download and process Parquet files as single table\n",
        "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
        "def download_parquet():\n",
        "    prefix = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata'\n",
        "\n",
        "    for month in range(1, 7):\n",
        "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "        yield df\n",
        "\n",
        "\n",
        "# Save current value and unset DESTINATION__CREDENTIALS to avoid conflict with DuckDB\n",
        "_original_credentials = os.environ.pop(\"DESTINATION__CREDENTIALS\", None)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"rides_pipeline\",\n",
        "    destination=\"duckdb\",  # Use DuckDB for testing\n",
        "    # destination=\"bigquery\",  # Use BigQuery for production\n",
        "    dataset_name=\"rides_dataset\",\n",
        ")\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "info = pipeline.run(download_parquet)\n",
        "\n",
        "# Print the results\n",
        "print(info)\n",
        "\n",
        "# Restore original value if it existed\n",
        "if _original_credentials is not None:\n",
        "    os.environ[\"DESTINATION__CREDENTIALS\"] = _original_credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDcLjzLtooBV",
        "outputId": "09889c75-d875-42ae-993d-5e907d857f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         database         schema                 name  \\\n",
            "0  rides_pipeline  rides_dataset           _dlt_loads   \n",
            "1  rides_pipeline  rides_dataset  _dlt_pipeline_state   \n",
            "2  rides_pipeline  rides_dataset         _dlt_version   \n",
            "3  rides_pipeline  rides_dataset                rides   \n",
            "\n",
            "                                        column_names  \\\n",
            "0  [load_id, schema_name, status, inserted_at, sc...   \n",
            "1  [version, engine_version, pipeline_name, state...   \n",
            "2  [version, engine_version, inserted_at, schema_...   \n",
            "3  [vendor_id, tpep_pickup_datetime, tpep_dropoff...   \n",
            "\n",
            "                                        column_types  temporary  \n",
            "0  [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME...      False  \n",
            "1  [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP W...      False  \n",
            "2  [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VAR...      False  \n",
            "3  [INTEGER, TIMESTAMP WITH TIME ZONE, TIMESTAMP ...      False  \n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# Set search path to the dataset\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "\n",
        "# Describe the dataset to see loaded tables\n",
        "res = conn.sql(\"DESCRIBE\").df()\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVJy8JoerI2P",
        "outputId": "63583f04-4784-4e0e-af68-07b43b4c6dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   count(1)\n",
            "0  20332093\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Save current value and unset DESTINATION__CREDENTIALS to avoid conflict with DuckDB\n",
        "_original_credentials = os.environ.pop(\"DESTINATION__CREDENTIALS\", None)\n",
        "\n",
        "# provide a resource name to query a table of that name\n",
        "with pipeline.sql_client() as client:\n",
        "    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n",
        "        data = cursor.df()\n",
        "print(data)\n",
        "\n",
        "# Restore original value if it existed\n",
        "if _original_credentials is not None:\n",
        "    os.environ[\"DESTINATION__CREDENTIALS\"] = _original_credentials"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}